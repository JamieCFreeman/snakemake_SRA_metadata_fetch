Building DAG of jobs...
Using shell: /usr/bin/bash
Provided cores: 1
Rules claiming more threads will be scaled down.
Job counts:
	count	jobs
	1	all
	1	combine_sra_table
	2	format_sra_table
	2	get_sra_table
	6

[Mon Jun  8 10:54:40 2020]
rule get_sra_table:
    output: metadata/SRR521288_sra_run_info.txt
    jobid: 5
    wildcards: project=SRR521288

[Mon Jun  8 10:54:51 2020]
Error in rule get_sra_table:
    jobid: 5
    output: metadata/SRR521288_sra_run_info.txt

RuleException:
CalledProcessError in line 12 of /local/workdir/jcf236/snakemake_SRA_metadata_fetch/Snakefile:
Command ' set -euo pipefail;  
		SRA=` echo metadata/SRR521288_sra_run_info.txt | sed 's ^.*/  ' | sed 's/_sra.*//' `;
			esearch -db sra -q ${SRA} | efetch -format runinfo > metadata/SRR521288_sra_run_info.txt ' returned non-zero exit status 255.
  File "/local/workdir/jcf236/snakemake_SRA_metadata_fetch/Snakefile", line 12, in __rule_get_sra_table
  File "/usr/lib64/python3.6/concurrent/futures/thread.py", line 56, in run
Removing output files of failed job get_sra_table since they might be corrupted:
metadata/SRR521288_sra_run_info.txt
Shutting down, this might take some time.
Exiting because a job execution failed. Look above for error message
Complete log: /local/workdir/jcf236/snakemake_SRA_metadata_fetch/.snakemake/log/2020-06-08T105439.983092.snakemake.log
